{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function [theta, J_history] = gradientDescentLinear(Xdata, y, theta, alpha, num_iters)\n",
    "%GRADIENTDESCENT Performs gradient descent to learn theta\n",
    "%   theta = GRADIENTDESENT(Xdata, y, theta, alpha, num_iters) updates theta by \n",
    "%   taking num_iters gradient steps with learning rate alpha\n",
    "% Input:\n",
    "%   Xdata- input data, size nxD\n",
    "%   Y- target Y values for input data\n",
    "%   theta- initial theta values, size Dx1\n",
    "%   alpha- learning rate\n",
    "%   num_iters- number of iterations \n",
    "%       Where n is the number of samples, and D is the dimension \n",
    "%       of the sample plus 1 (the plus 1 accounts for the constant column)\n",
    "% Output:\n",
    "%   theta- the learned theta\n",
    "%   J_history- The least squares cost after each iteration\n",
    "\n",
    "% Initialize some useful values\n",
    "n = length(y); % number of training examples\n",
    "J_history = zeros(num_iters, 1);\n",
    "\n",
    "for iter = 1:num_iters\n",
    "    \n",
    "    theta0 = theta(1) - (alpha/n)*sum(Xdata*theta-y);\n",
    "    theta1 = theta(2) - (alpha/n)*sum((Xdata*theta-y).*Xdata(:,2));\n",
    "    \n",
    "    theta = [theta0 ; theta1];\n",
    "    \n",
    "    % Save the cost J in every iteration    \n",
    "    J_history(iter) = computeCost(Xdata, y, theta);\n",
    "\n",
    "end\n",
    "\n",
    "end\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
